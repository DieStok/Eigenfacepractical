---
title: "Eigenfaces practical"
author: "Dieter Stoker"
date: "8-9-2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#unsupervised learning day 1

You've learned about the importance of clustering and dimensionality reduction. Let's put them into practice. Before we dive into a biological example, let's use something that our brains are especially well-wired to interpret and think about: faces.

The question is: can we take pictures of human faces, run them through dimensionality reduction, and still keep most of the, well, face-ness intact? Start by running the code below to import images of faces (specifically, from here: http://vis-www.cs.umass.edu/lfw/ , and within that data only those people whose names start with a).

```{r}
#this installs pacman: using its p_load function you can load any package you want, and if you don't have it installed it will automatically download it for you.
if(!require(pacman)) {
install.packages("pacman"); require(pacman)}
#load packages we want to use
p_load(ggplot2, tidyverse, EBImage, stringr, devtools, R.matlab)

#some packages are not on CRAN, so need to be downloaded manually
install_github("vqv/ggbiplot")

#get the file names
faceDir = paste0(getwd(), "/Faces")
fileNames = list.files(path =faceDir, pattern=".*.jpg")
str(fileNames)

#read in the images
imageList = lapply(paste0(faceDir, "/", fileNames), readImage)
str(imageList)



```

Great, we have images. Let's view some.

```{r}

#display 15 images
for (i in seq(15)) {
  
  randomImageIndex = sample(seq_along(imageList), 1)
  imageToDisplay = imageList[[randomImageIndex]]
  nameImage      = fileNames[randomImageIndex]
  print(nameImage)
  display(imageToDisplay, "raster")
  
}

```


For our dimensionality reduction to be more manageable, let's make the images grayscale.

```{r}
greyImageList = list(length(imageList))
for (i in seq_along(imageList)) {
  

  greyImageList[[i]] <- imageData(channel(imageList[[i]], "gray"))
  
}


```

See how that looks.

```{r}
for (i in seq(10)) {
  
  randomImageIndex = sample(seq_along(greyImageList), 1)
  imageToDisplay = greyImageList[[randomImageIndex]]
  nameImage      = fileNames[randomImageIndex]
  print(nameImage)
  display(imageToDisplay, "raster")
  
}
```

Now, up to you to question the data somewhat. Answer the following questions using code you write below:

Q1: How many data points are there?
Q2: How many unique people are in this dataset?
Q3: Whose face is most represented in this dataset, and how many times is that face represented?
Q4: What does the distribution of counts look like? (use hist)

**Hint: you can use functions like str_replace_all to change '.jpg' into an empty space ''. If you do the same for numbers and underscores, you are left with only the names. Then you can use the unique() function and table() along with which.max() to get the answers**

**Hint: to make a histogram, use R's built-in hist() function**

```{r}

#demonstrations of the functions you can use:


#replace numbers and underscores

someString = "Billy_Jean_Is_Not_my_Love_995599.mp3"
noUnderscores = str_replace_all(someString, "_", "")
noExtension = str_replace_all(noUnderscores, "\\.mp3$", "")
noNumbers = str_replace_all(noExtension, "[0-9]", "")
#You can also use other methods like str_extract or intricate regexp queries, if you know them. If not, these 3
#ingredients should get the result you want!


#investigating how many entries there are of specific types and unique entries

someVector = c("banana", "apple", "grape", "apple", "plum", "banana", "apple", "Weird Al", "Stegosaurus", "plum")
print(someVector)
#get unique entries
print(unique(someVector))
#count and tabulate entries
print(table(someVector))
#get the index of the element in the table with the maximum value 
print(which.max(table(someVector)))
#print that element
print(table(someVector)[which.max(table(someVector))])


#Now it's up to you, get the names of the people whose faces are in the data, see how many unique people
#there are, tally how many times each person is in there and make a histogram, and see who is in there the most

##ANSWER##

print(paste0("Amount of data points: ", length(fileNames)))
namesWithUnderscore = str_extract(fileNames, "[A-Za-z-]*_*[A-Za-z-]*_*[A-Za-z-]*_*[A-Za-z-]*_*")
namesNoUnderscore = str_replace_all(namesWithUnderscore, "_", "")
length(unique(namesNoUnderscore))
tableNames = table(namesNoUnderscore)
mostEntriesIndex = which.max(tableNames)
print(paste0("Most entries: ", tableNames[mostEntriesIndex], " ", names(tableNames[mostEntriesIndex])))
hist(tableNames)

```




Now that you've gotten to know the data some, let's do what we set out to do. First, let's think about the dimensionality of the input space. You can use the code block below to answer these questions.

Q5: What do the numbers printed in the matrix signify? 
Q6: What is the dimension of the input space?
Q7: Do you think that this n*n-dimensional space is evenly (or randomly) filled with examples from this data, or not? Why?
Q8: Do you think it is a logical step to perform PCA, a linear dimension reduction method, on this (type of) data?



```{r}
someImage = sample(greyImageList, 1)[[1]]
print(someImage)
dim(someImage)

```




Regardless of your feelings in question 8, we will soldier on and do the PCA. Remember that PCA does not, in and of itself, do dimension reduction: it just creates orthogonal axes through your data, starting from the axis that captures most variance, to the next most variance, etc. In other words: there are as many of these axes as there are dimensions in your data. It's up to you to select the top n principal components to keep. As a first step, we need to get our data in a form useful for PCA. PCA is often performed via something called singular value decomposition (details for the mathematically inclined here: https://stats.stackexchange.com/questions/189822/how-does-centering-make-a-difference-in-pca-for-svd-and-eigen-decomposition ), and this requires data with a zero-mean. This has a cool side effect: we need to calculate the 'average face' and subtract it from the data. Up to you to do this and uncover the ghostly mean face!

Q9: in the code block below, compute the mean value of every pixel over all images. Then, visualise this face by displaying the matrix.
Q10: in the code block below, make sure each row has 0 mean by subtracting the rowMeans from the data. (So you subtract the mean grey intensity of pixel 1 across all images from pixel 1 across all images). To check that you did it correctly, calculate the rowMeans() again. They should be zero (or close to it: e^(-17) is close enough). 

**Hint: use matrix() (as shown below), rowMeans(), and display(YOURAVERAGEFACEHERE, method = "raster")**

```{r}

#step 1: each image is a 250*250 square matrix. But for PCA we need something like a data table, with samples in the columns, and values in the rows, i.e. like:

exampleData = data.frame("face1" = c( 0.55,0.33,0,0.56), "face2" = c(0.786,0.257,1, 0))
rownames(exampleData) = c("pixel1", "pixel2", "pixel3", "pixel62500")
exampleData

#You can easily switch between a vector and a matrix
as.vector(someImage)
matrix(as.vector(someImage), nrow = 250, ncol = 250)

#put every image in a data frame
greyImageDataFrame = data.frame(matrix(nrow = length(someImage), ncol = length(fileNames)))
for (imageMatrixIndex in seq_along(greyImageList)) {
  
  greyImageDataFrame[,imageMatrixIndex] = as.vector(greyImageList[[imageMatrixIndex]])
  
}
rownames(greyImageDataFrame) = paste0(rep("pixel", length(someImage)), seq(1,length(someImage)))
colnames(greyImageDataFrame) = paste0(rep("face", length(imageList)), seq(1,length(imageList)))
#over to you. We now have a 62500*1054 data.frame with pixel data. Calculate the average face, display it, and subtract the average values from the data.


##ANSWER##

#Computing the 'average' face in these datasets.
avgFace = rowMeans(greyImageDataFrame)

#So, how does an average grayscale face look?

avgFaceMatrix = matrix(avgFace, nrow = 250, ncol = 250)
display(avgFaceMatrix, "raster")

#Subtracting the average face
meanSubtractedGreyImageDataFrame = greyImageDataFrame - avgFace

```


Okay, with that done we can do our PCA. We'll pare our set down to 256 faces to speed up calculation. The beauty of this approach is that we can visualise what, exactly, PCA is doing. Normally, we cannot. Go ahead and run the following code. In it, we perform PCA, and then look at the 10 eigenvectors with the largest eigenvalues, or in less technical terms: the 10D representation of the data that captures most of the variance. Since each eigenvector is 62500-dimensional, you can simply put it back into the form of an image and look at it, to see what each image represents. In other words: you can look at the largest variance from the mean face, and then the second largest, etc. in this dataset.


```{r}

#step 1: we'll randomly remove 75% of the images: otherwise things will take a LONG time to calculate

sampledFaceIndices = sample(seq(1:1024), 1024/4)
smallerDataFrame = meanSubtractedGreyImageDataFrame[, sampledFaceIndices]



facePCA = prcomp(smallerDataFrame, scale = TRUE)
print(summary(facePCA))

#What do the principal components look like? What does a high or low value on these principal components correspond to? Let's find out!

#get everything in matrix form
fullComponentMatrixList = list(ncol(facePCA$x))
for (component in seq(1:ncol(facePCA$x))) {
  
  fullComponentMatrixList[[component]] = matrix(facePCA$x[, component], nrow = 250, ncol = 250)
}



for (component in seq(1:10)) {
  
  #draw it --> drawing logic assumes a different ordering of the matrix so we have to flip it, see: https://stackoverflow.com/a/66453734 
  image(fullComponentMatrixList[[component]][, nrow(fullComponentMatrixList[[component]]):1],
        col = gray.colors(256, 0, 1))
  
}

```



Q11: First, scroll up to look at the normal input faces we fed into our PCA again. Given the variability in these faces, what do you think the first five principal components might roughly correspond to? Think of orientation (left-right and where the face is in the image) and some prominent facial features. (Note: this question is hard to do wrong, but also hard to do exactly right).

Q12: The first principal component is the dimension along which faces in this dataset vary the most from the mean. Why do you think that it looks as it does?


PCA is dimensionality reduction. So let's see: in how many dimensions can we put our data to still capture most of the variance? Run the following code and look at the plots, then answer the questions below.


```{r}

#How much variance in these faces does each principal component have?

#calculate total variance explained by each principal component
varExplained = facePCA$sdev^2 / sum(facePCA$sdev^2)

#create scree plot
qplot(c(1:length(varExplained)), varExplained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot (percentage of variance explained by each PC)") +
  ylim(0, 1) +
  theme_bw()

#Create cumulative variance explained plot
cumulVar <- cumsum(varExplained)
plot(cumulVar[0:50], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot (zoom-in until 75%)")
abline(h = 0.5, col="blue", lty=5)
abline(v = 10, col="blue", lty=5)
abline(h = 0.75, col="red", lty=5)
abline(v = 40, col="red", lty=5)
legend("bottomright", legend=c("Cut-off @ PC10", "Cut-off @ PC40"),
       col=c("blue", "red"), lty=5, cex=0.6)


#what if we really wanted 90% of the variance?
plot(cumulVar[0:100], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot (up to 90%)")
abline(h = 0.9, col="green", lty=5)
abline(v = 100, col="green", lty=5)

legend("bottomleft", legend=c("Cut-off @ 100"),
       col=c("green"), lty=5, cex=0.6)


#print(facePCA$center)
#print(facePCA$stdev)


```
Q13 How many dimensions do you need to keep 50% of the variance in the face data? And 75%?

Q14 By what factor can you decrease the dataset size and still keep 90% of the variance? Remember, you started with 256 faces, each of which needed 62500 datapoints to describe. But by rotating things around, you can shrink from 256 to fewer values to describe this data, and that still represents 90% of the variation in the original 256 unique images.

***A: you compressed the data by a factor of 2.56 ***


Now what we can do is try to reconstruct our original faces from the values they have in the compressed space (and by adding the mean face back in). Let's see how that looks for a changing number of principal components used. Run the two code blocks below and look at the results.



```{r, fig.width=7, fig.height=7}








#take a random face from the 256 that we did the PCA on
randomImageIndex = sample(sampledFaceIndices, 1)
faceNumberInPCAObject = which(sampledFaceIndices == randomImageIndex)
imageToDisplay = greyImageList[[randomImageIndex]]
nameImage      = fileNames[randomImageIndex]



#make more specific faces continuously, by adding more and more PCs

steps = c(1,2,3,4,5,6,7,8,9,10, seq(20, 240, 20), 256)

image(avgFaceMatrix[, nrow(avgFaceMatrix):1],col = gray.colors(256, 0, 1),
        main = paste0("Average Face"))
for (PCsToAdd in steps[seq_along(steps)]) {
  
  currentReconstruction = avgFaceMatrix
  for (matrixIndex in seq_along(fullComponentMatrixList[1:PCsToAdd])) {
    currentReconstruction = currentReconstruction + facePCA$rotation[faceNumberInPCAObject, matrixIndex] *
      fullComponentMatrixList[1:PCsToAdd][[matrixIndex]]
  }
  
  
image(currentReconstruction[, nrow(currentReconstruction):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Reconstruction using ", PCsToAdd, " Principal Components" ))
  
}

#print normal face
print(nameImage)
image(imageToDisplay[, nrow(imageToDisplay):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Original" ))



```


Note that the slight discrepancy between the 256-PC reconstruction and the original image is because of a normalisation of the variance in every pixel: if pixel 1 only ever has values between 0.6 and 0.7, while another can vary between 0.2 and 1, then the second pixel would automatically have 'more' variance, so influence the PCA more, but that is not, per se, fair. 


We want to use PCA not to keep all dimensions and reconstruct things. That may be fun, but it is also silly. How about actually reducing the dimensions? Can we see anything in the 2D representation of this data? Where, for example, is our most numerous gentleman, Ariel Sharon, in this representation? 

```{r}

imageIndicesSharon = which(namesNoUnderscore == "ArielSharon")
sampledDataPointsSharon =  which(sampledFaceIndices %in% imageIndicesSharon)

dataWithSharonNotSharon = data.frame(facePCA$rotation[,1:2])
dataWithSharonNotSharon$Person = "Not Sharon"
dataWithSharonNotSharon[sampledDataPointsSharon,"Person"] = "Sharon"

plot2D = ggplot(data =dataWithSharonNotSharon,
           aes(x = PC1, y = PC2, color = Person)) +
           geom_point() +
           theme_bw()

show(plot2D)

```

Seems like Ariel Sharon is hanging out mostly in one area. 

##THIS MIGHT GO WRONG BECAUSE OF THE RANDOM SEED!##
Q15 Let's look at two pictures featuring Sharon from the Sharon-region, and some outlying Sharon-points. Is there any difference that catches your eye, or not?

##Answer: no, not really. It doesn't seem like they are that different. It's not that easily interpretable, judging these images along differences in just 1 principal component that accounts for ~12% of the variation.##

```{r, fig.width=4, fig.height=4}
outlierRows = dataWithSharonNotSharon[sampledDataPointsSharon, ][dataWithSharonNotSharon[sampledDataPointsSharon, ]$PC1 < 0,]
sharonOutlierImageIndices = as.numeric(str_replace_all(rownames(outlierRows), "face", ""))

for (index in sharonOutlierImageIndices) {
  image(greyImageList[[index]][, nrow(greyImageList[[index]]):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Sharon Outlier" ))
}

normalRows = dataWithSharonNotSharon[sampledDataPointsSharon, ][dataWithSharonNotSharon[sampledDataPointsSharon, ]$PC1 >= 0,]
sharonNormalImageIndices = as.numeric(str_replace_all(rownames(normalRows), "face", ""))

for (index in sample(sharonNormalImageIndices, length(sharonNormalImageIndices)/2)) {
  image(greyImageList[[index]][, nrow(greyImageList[[index]]):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Sharon Normal" ))
}


```



Now, there's actually quite a lot of variation in the faces in this dataset. Let's quickly check how PCA fares in another dataset with faces pictured from the front.

```{r, fig.width=2, fig.height=2}

centeredFacesDataset = R.matlab::readMat("./CenteredFaces/faces.mat")

image(t(centeredFacesDataset$face[[2]][[1]])[, ncol(t(centeredFacesDataset$face[[2]][[1]])):1],
        col = gray.colors(256, 0, 1))




```




























This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
