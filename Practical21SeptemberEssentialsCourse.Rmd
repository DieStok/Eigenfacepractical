---
title: "Eigenfaces practical"
author: "Dieter Stoker"
date: "8-9-2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#####
##
## FIRST: Download RTools
#Windows: https://cran.r-project.org/bin/windows/Rtools/ , read there and see: https://jtleek.com/modules/01_DataScientistToolbox/02_10_rtools/#1 
#Mac: https://github.com/rmacoslib/r-macos-rtools 
#Linux: don't know exactly, run the chunk below installing things and see how it goes, we'll troubleshoot errors
##
#####



#unsupervised learning day 1

#part 1: clustering

We'll start by performing some K-means clustering. The code block below generates some random data with which to perform the clustering. We colour the points according to the pre-set cluster they are supposed to belong to, based on the signal we generate in the data.
```{r}
##SETUP CHUNK: INSTALLS AND LOADS NEEDED PACKAGES##

#this installs pacman: using its p_load function you can load any package you want, and if you don't have it installed it will automatically download it for you.
if(!require(BiocManager)) {
install.packages("BiocManager"); require(BiocManager)}
if(!require(pacman)) {
install.packages("pacman"); require(pacman)}
#load packages we want to use
p_load(ggplot2, magrittr, plyr, tidyverse, EBImage, stringr, devtools, R.matlab, gganimate, png, gifski, knitr)

#some packages are not on CRAN, so need to be downloaded manually
install_github("vqv/ggbiplot")

#options(dplyr.summarise.inform = FALSE)

```


```{r}

meanX = c(2,2.5,8,7.3)
meanY = c(2, 7.9, 2.33, 9)
sds   = c(1, 1.5, 1.3, 0.85)
nPoints = rep(25, 4)
clusterIdentity = c(rep("1", 25), rep("2",25), rep("3",25), rep("4",25))

xValues = purrr::pmap(list(nPoints, meanX, sds), rnorm)
yValues = purrr::pmap(list(nPoints, meanY, sds), rnorm)

dataforClustering = as_tibble(list("Gene 1 Expression" = unlist(xValues),
                                    "Gene 2 Expression" = unlist(yValues),
                                   "True Cluster" = as.factor(clusterIdentity)))


ggplot(data = dataforClustering, aes_string(x = "`Gene 1 Expression`", y = "`Gene 2 Expression`", colour = "`True Cluster`")) + geom_point(size = 3) +
  theme_bw() 
  
  

```
Now it is time to perform some clustering on this data. Run the code block below, and answer the questions. We perform k-means until convergence is reached (the centroids don't move anymore) for 4 different random initialisations of 4 clusters.


```{r}

#####
## TO DO: PORT THESE FUNCTIONS TO ANOTHER FILE AND LOAD THAT IN, RATHER THAN HAVING THEM HERE?
####

#implement own k-means function
doKMeans = function(data, startCentroids, k = 4, maxIterations = 10, distance = "euclidean", stopThreshold = 0.001) {
  centroidList = vector("list", (maxIterations+1))
  clusterMembershipList  = vector("list", (maxIterations+1))
  currentCentroids = startCentroids
  
  dimensions = ncol(data)
  dataWithClusterAssignmentPerIteration = data
  
  for (i in seq(1:maxIterations)) {
    
    
    
    distancesList = vector("list", length = k)
    for (clusterKDistance in seq(1:k)) { 
      
      
      distVec = numeric(length = nrow(data))
      if (distance == 'euclidean') {
        
        for (row in seq(1:nrow(data))) {
          distThisRow  = rowSums((data[row, ] - currentCentroids[clusterKDistance, ])^2)
          distVec[row] = distThisRow
        }
        
      }
      
      distancesList[[clusterKDistance]] = distVec
      
    }
    #assign all points to the centroid they are closest to and save this assignment in a list
    distancesToAllCentroids = dplyr::bind_cols(distancesList)
    clusterAssignment       = apply(distancesToAllCentroids, 1, which.min)
    
    #add in initial situation
    if (i == 1) {
      clusterMembershipList[[1]] = clusterAssignment
      centroidList[[1]]          = startCentroids
      dataWithClusterAssignmentPerIteration[[paste0("clusterMembershipIteration", 0)]] = factor(clusterAssignment, levels = as.character(seq(1:k)))
    }
    
    
    dataWithClusterAssignmentPerIteration[[paste0("clusterMembershipIteration", i)]] = factor(clusterAssignment, levels = as.character(seq(1:k)))
    clusterMembershipList[[i+1]] = clusterAssignment
    
    
    #recalculate the cluster centroids and save this assignment in a list
    newCentroids = dataWithClusterAssignmentPerIteration %>%
      group_by(across(all_of(paste0("clusterMembershipIteration", i)))) %>%
                 summarize(across(.cols = 1:dimensions, mean))
    currentCentroids = newCentroids[, 2:(dimensions+1)]
    centroidList[[i+1]] = currentCentroids  
    
    
    
    
    
    #check for convergence. If total difference between centroid coordinates between iterations is
    #below certain threshold, stop.
    if (i > 1) {
      
      differenceInCentroidsBetweenIterations = abs(sum(centroidList[[i]] - centroidList[[i-1]]))
      
      if(differenceInCentroidsBetweenIterations < stopThreshold) {
        break
      }
      
    }
    
    
  }
  list(dataPointAssignedClusters = purrr::compact(clusterMembershipList),
       clusterCentroidPositions  = purrr::compact(centroidList),
       dataAllClusterAssignment  = dataWithClusterAssignmentPerIteration)
 
  
}


#plot initial situation + true cluster membership
preClusterPlot   = function(data, centers) {
  bgPlot = ggplot(data = dataforClustering, aes_string(x = "`Gene 1 Expression`", y = "`Gene 2 Expression`", colour = "`True Cluster`")) +
  geom_point(size = 3) +
  geom_point(size = 5, stroke = 2.5, shape = 21, data = centers, colour = "black",
             aes_string(fill = "`True Cluster`"),
             show.legend = FALSE) +
  scale_fill_discrete(drop = FALSE) +
  theme_bw() +
    ggtitle("True underlying data and starting centroids")
  bgPlot
}

#plot the cluster membership and centroids for each iteration of k-means
iterationPlot = function(dataPoints, dataCenters) {
  
  #make plots according to number of iterations in data
  plotsToMake = dataPoints %>% select(contains("Iteration")) %>% ncol()
  colNames = dataPoints %>% select(contains("Iteration")) %>% colnames()
  itPlotList = vector("list", length = plotsToMake)
  for (plotNr in seq(1:plotsToMake)) {
    
    dataWithCentroidIdentity = dataCenters[[plotNr]] %>% mutate(Centroid = as.factor(1:nrow(dataCenters[[plotNr]])))
    itPlot = ggplot(data = dataPoints,
       aes_string(x = "`Gene 1 Expression`", y = "`Gene 2 Expression`",
                  colour = colNames[plotNr])
                  ) +
  geom_point(size = 3) +
  geom_point(size = 5, stroke = 2.5, shape = 21, data = dataWithCentroidIdentity, colour = "black",
             aes_string(fill = "Centroid"),
             show.legend = NA) +
  theme_bw() +
      ggtitle(paste0("iteration: ", plotNr))
  itPlotList[[plotNr]] = itPlot
    
  }
  
  itPlotList
}

#make it animated
iterationPlotAnim = function(dataPoints, dataCenters) {
  
  #reshape data for animation
  nIterations                 = dataPoints %>% select(contains("Iteration")) %>% ncol()
  dimData                     = dataPoints %>% select(-contains("Iteration")) %>% ncol()
  dataPointsLong              = dataPoints %>% pivot_longer((dimData+1):(nIterations+dimData)) %>% arrange(name)
  dataPointsLong$name         = as.factor(readr::parse_number(dataPointsLong$name))
  colnames(dataPointsLong)[3] = "iteration"
  colnames(dataPointsLong)[4] = "cluster membership"
  #dataPointsLong %<>% select(-iteration)
  
  #reshape centroids for animation
  centroidFrame           = dplyr::bind_rows(dataCenters)
  centroidFrame$iteration = as.factor(rep((seq(1:(nIterations))-1), each = nrow(dataCenters[[1]])))
  centroidFrame$centroid  = as.factor(rep(seq(1:nrow(dataCenters[[1]])), (nIterations) ))

    
    itPlot = ggplot(data = dataPointsLong,
       aes_string(x = "`Gene 1 Expression`", y = "`Gene 2 Expression`",
                  colour = "`cluster membership`")
                  ) +
  geom_point(size = 3) +
  geom_point(size = 5, stroke = 2.5, shape = 21, data = centroidFrame, colour = "black",
             aes_string(fill = "centroid"),
             show.legend = NA) +
  theme_bw() +
      transition_manual(`iteration`) +
      ease_aes() +
      labs(title = "Iteration: {current_frame}")
  itPlot
}

#start with randomStarts random initialisations, picking k random centroids from the data each time
getKRandomStartPoints = function(data, k = 4, randomStarts = 4) {
  
  centersForRuns = vector("list", randomStarts)

for( i in seq_along(centersForRuns)) {
  centersForRuns[[i]] = data[sample(seq(1:nrow(data)), k), ]
}
  centersForRuns
  
}

#calculate kMeans clustering steps for each random initialisation, output in list
#note: in this form, function won't work for multi-dim data
calculateKMeansTrajectoryEachRandomInit = function(data = dataforClustering, initialCentroids = centersForRuns) {
  
  kMeansOutcomes = vector("list", length = length(initialCentroids))
  for(randomCentroidInitialisationNr in seq_along(initialCentroids)) {
    kMeansOutcomes[[randomCentroidInitialisationNr]] = doKMeans(data[, 1:2],
                                                              initialCentroids[[randomCentroidInitialisationNr]][, 1:2])
  }
  kMeansOutcomes
}

#generate plots and animations using the plotting and animating function defined above
makePlotsAndAnimation = function(kMeansOutcomeData = kMeansOutcomes) {
  
  animationList = vector("list", length(kMeansOutcomeData))
  totalPlotList = vector("list", length(kMeansOutcomeData))
  for(i in seq_along(kMeansOutcomeData)) {
  
    beginPlot      = preClusterPlot(data = dataforClustering, centers = centersForRuns[[i]])
    iterationPlots = iterationPlot(dataPoints  = kMeansOutcomeData[[i]]$dataAllClusterAssignment,
                               dataCenters = kMeansOutcomeData[[i]]$clusterCentroidPositions)
    animationPlot  = iterationPlotAnim(dataPoints  = kMeansOutcomeData[[i]]$dataAllClusterAssignment,
                                dataCenters = kMeansOutcomeData[[i]]$clusterCentroidPositions)
    totalPlotList[[i]] = list(beginPlot)
    totalPlotList[[i]] = append(totalPlotList[[i]], iterationPlots)
    animationList[[i]] = animationPlot
  
  }
  
  return(list(plots = totalPlotList, animations = animationList))
  
}

centersForRuns = getKRandomStartPoints(data = dataforClustering, k = 4, randomStarts = 4)

kMeansOutcomes = calculateKMeansTrajectoryEachRandomInit(data = dataforClustering,
                                                         initialCentroids = centersForRuns)

plotsAndAnimations = makePlotsAndAnimation()

```

```{r}
#random initialisation 1 and 2
plotsAndAnimations$plots[[1]]
print("---")
plotsAndAnimations$plots[[2]]
```


```{r}
#random initialisation 3 and 4
plotsAndAnimations$plots[[3]]
print("---")
plotsAndAnimations$plots[[4]]
```

```{r}
#animations --> change the number to see a different one
plotsAndAnimations$animations[[2]]
```

Q1: Look at the clusters produced. How many get, or almost get, the 'real' clustering that produced the data?

#Answer: this depends on your random initialisation, which we haven't set to a certain seed on purpose to keep things interesting and different for discussion. About 2 converged on 4 clusters in our case.

Q2: In the cases where a 'wrong' clustering was obtained, what caused this?

#Answer: in general what will happen is that a cluster centroid gets 'stuck' behind another. For example, if three centroids are, at random, initialised in the same real cluster, then it happens that 1 centroid covers two clusters, clusters, while the others divvy up the remaining points between them. This is exactly why you need multiple initialisations!

Q3: We actually cheated here, in the sense that we know the underlying data structure, and adapted our k to it (4). In reality, we wouldn't know this (although in this 2D data, we could use our visual system!). Try k-values from 2 to 8, and see what sort of clusters you get. Use the code chunk below and the functions defined above. Feel free to make extra code blocks as needed. What do you see?




```{r}
#Use these functions:
centersForRunsKTwo = getKRandomStartPoints(data = dataforClustering, k = 2, randomStarts = 4)

kMeansOutcomesKTwo = calculateKMeansTrajectoryEachRandomInit(data = dataforClustering,
                                                         initialCentroids = centersForRunsKTwo)

plotsAndAnimationsKTwo = makePlotsAndAnimation(kMeansOutcomeData = kMeansOutcomesKTwo)


```


Q4: having seen this diversity, try to think of a way you could try to quantify how good a given k-means clustering is. Think about how well each point in a cluster 'fits' or belongs to that cluster versus how well-separated the clusters are (how far the centroids are away from each other).

#Answer: What you'd want to do is do exactly what we discussed in the lecture: make sure that points in a cluster have a low spread, while there is a relatively larger spread between the clusters. That is: you want a cluster to be a tight spread of points, and ideally to be far away from any other cluster. 

Q5: What would the optimal number of clusters be for n datapoints if you only want to minimise the distance from a point to the centroid of the cluster it belongs to? Is this useful?

#Answer: That would be k=n, in other words: then you're not clustering. In this case, the distance from each point to its centroid is 0. That's perfect, but useless.

We've talked during the lecture about problems in higher dimensions. You can cluster in higher dimensions and then project this high-dimensional data down to 2D for plotting, plotting the clusters as well. We've covered, however, that once the number of dimensions gets large, distance metrics lose their meaning.

Before we move into showing that outright, let's add noisy genes to our data, reduce the dimensions with PCA, and plot the 2D plot. Answer the questions under the code block.

```{r}
#Here we will set the seed so things are equal, so you can discuss with your neighbours!
set.seed(424242)


#plot of original data
ggplot(data = dataforClustering, aes_string(x = "`Gene 1 Expression`", y = "`Gene 2 Expression`", colour = "`True Cluster`")) + geom_point(size = 3) +
  theme_bw() 


#just the 2D data
pca2DData = prcomp(dataforClustering %>% select(-"True Cluster"), center = TRUE, scale. = TRUE)

#plot it
ggbiplot::ggbiplot(pca2DData, groups = dataforClustering$`True Cluster`, varname.adjust = 1.1, varname.abbrev = TRUE) + theme_bw()

#add noise to our data
dataWithNoise = dataforClustering
dataWithNoise[["Gene 3 Expression"]] = runif(nrow(dataWithNoise), 0, 8)

#perform PCA
pca3DData = prcomp(dataWithNoise %>% select(-"True Cluster"), center = TRUE, scale. = TRUE)

#plot data
ggbiplot::ggbiplot(pca3DData, groups = dataWithNoise$`True Cluster`, varname.adjust = 1.1, varname.abbrev = TRUE) + theme_bw()


#add another dimension with noise and do this again
dataWithNoise[["Gene 4 Expression"]] = runif(nrow(dataWithNoise), 0, 8)

pca4DData = prcomp(dataWithNoise %>% select(-"True Cluster"), center = TRUE, scale. = TRUE)

ggbiplot::ggbiplot(pca4DData, groups = dataWithNoise$`True Cluster`, varname.adjust = 1.1, varname.abbrev = TRUE) + theme_bw()

#Once more because it's cool
dataWithNoise[["Gene 5 Expression"]] = runif(nrow(dataWithNoise), 0, 8)

pca5DData = prcomp(dataWithNoise %>% select(-"True Cluster"), center = TRUE, scale. = TRUE)

ggbiplot::ggbiplot(pca5DData, groups = dataWithNoise$`True Cluster`, varname.adjust = 1.1, varname.abbrev = TRUE) + theme_bw()

```


Q6: What has happened in the first PCA plot, where we 'reduce' dimensions from 2 to 2? What do the arrows mean?

#Answer: the data has been rotated, but is otherwise unperturbed. The arrows are the 2D projections of the original variable axes in the new space. In this case, they just correspond to the x- and y-axis in the original plot.

Q7: What happens to the clusters (as we predefined them) when we add more noise? Why?

#Answer: they become less and less well-defined in the dimension reduction, because more and more of the variance in the data is due to the noisy variables we added. The points start to become more like each other in noisy dimensions rather than in the 2 dimensions with signal that we predefined (the 4 groups in expression in gene 1 and 2).

Q8: in the code block below, run ggbiplot::ggscreeplot() on the output of the different PCAs. How many components do you need in each case to get >~ 80% of the variability in the data?

#answer: first one, 2. Second one: 3, fourth one: 3, fifth one: 4

```{r}

```




Now, let's run kmeans on the multidimensional data, and then show the groups found in the 2D space. Note that R's k-means function automatically does multiple initialisations and chooses the 'best' clusters (it is smarter than the basic k-means explained in the lectures) 

Run this procedure also for the 4D and 5D data. So:
-select the columns you need
-perform k-means
-plot a ggbiplot where you colour the actual clusters ("True Cluster" column)
-plot a ggbiplot where you colour the clusters k-means found 

Below is the example for the 2D data. You must do it for the 4D and 5D data!

```{r}
k = 4
kMeans2D = kmeans(dataforClustering %>% select(-"True Cluster"), centers = k)


#plot 2D plot with actual clusters
ggplot(dataforClustering %>% mutate(kMeansCluster = as.factor(kMeans2D$cluster)),
       aes( x = `Gene 1 Expression`, y = `Gene 2 Expression`, colour = `True Cluster`)) +
  geom_point() + 
  theme_bw() +
  ggtitle("Actual clusters")

#plot 2D plot with k-means clusters
ggplot(dataforClustering %>% mutate(kMeansCluster = as.factor(kMeans2D$cluster)),
       aes( x = `Gene 1 Expression`, y = `Gene 2 Expression`, colour = kMeansCluster)) +
  geom_point() + 
  theme_bw() +
  ggtitle("K-means clusters")


#plot PCA plot with actual clusters
ggbiplot::ggbiplot(pca2DData, groups = as.factor(dataforClustering$`True Cluster`), varname.adjust = 1.1, varname.abbrev = TRUE) + theme_bw() +
  ggtitle("Actual clusters PCA")

#plot PCA plot with k-means cluster
ggbiplot::ggbiplot(pca2DData, groups = as.factor(kMeans2D$cluster), varname.adjust = 1.1, varname.abbrev = TRUE) + theme_bw() +
  ggtitle("K-means clusters PCA")


#################################################
##
##          UP TO YOU FROM HERE!
##
#################################################


#functions you need. If you don't understand how to use them, execute ?FUNCTION_NAME in the console, or search online!

columnsToKeep = c("Gene 1 Expression", "Gene 2 Expression", "Gene 3 Expression", "Gene 4 Expression")
dataWithNoise %>% select(contains(columnsToKeep)) 
?kmeans()



####
##
## ANSWER
##
####

keepUntil = c(4,5)
listPCAResults = list(`4` = pca4DData, `5` = pca5DData)
for (i in seq_along(keepUntil)) {
  
  columnsToKeep = paste0("Gene ", seq(1,keepUntil[i]), " Expression")
  dataND        = dataWithNoise %>% select(contains(columnsToKeep)) 
  kMeansND      = kmeans(dataND, centers = k)
  
  
  #plot PCA plot with actual clusters
  realClusterPlot = ggbiplot::ggbiplot(listPCAResults[[as.character(keepUntil[i])]], groups = as.factor(dataforClustering$`True Cluster`), varname.adjust = 1.1, varname.abbrev = TRUE) +
    theme_bw() +
  ggtitle("Actual clusters PCA")

#plot PCA plot with k-means cluster
  kMeansPlot = ggbiplot::ggbiplot(listPCAResults[[as.character(keepUntil[i])]],
                                  groups = factor(kMeansND$cluster, levels = c(1,2,3,4)), varname.adjust = 1.1, varname.abbrev = TRUE) +
    theme_bw() +
  ggtitle("K-means clusters PCA")
  show(realClusterPlot)
  show(kMeansPlot)
  
}





```

Q9: What do you see? Do the clusters still resemble the 'actual clusters' that we put in?

#Answer: yes, for these uniform noise dimensions which were added, the clusters are still exactly the same as we defined them. K-means isn't disturbed by the noise so far.

  

Now let's add more noisy dimensions and see what happens with clustering. We'll take our data from above, and just start adding more and more random noisy genes. Then we'll do PCA and visualise the first 2 components each time. I say 'we', but it's going to be you. So:

-Add 10, 20, 50, 100 and 200 noise dimensions (save each as a different dataframe)
-Perform pca on each dataframe (use prcomp(data, center = TRUE, scale. = TRUE ))
-Perform kmeans clustering on each dataframe (use R's kmeans() function), use k = 4
-Plot the 2 first principal components (use ggbiplot::ggbiplot(), like above), once colouring the groups by the predefined signal (Gene 1 and Gene 2), and once colouring them by the clusters that k-means arrives at.

--> Feel free to use a loop, but also feel free not to use loops!
--> If not using loops, it's fine if you just do 2 or 3 cases (20, 50, 200 noise dimensions)
--> You can check the code blocks above for the type of thing you need to do here (it uses similar functions and data editing techniques)
--> If you get stuck, feel free to check the answers.


```{r}
#functions you might want to use/example of adding random noise
exampleData     = tibble(`Gene 1 Expression` = c(1,2,3),`Gene 2 Expression` = c(4,5,6))
colsToAdd       = 5
randData        = purrr::map(rep(nrow(exampleData), colsToAdd), runif, 0, 8)
newNames        = paste0("Gene ", seq(3,(colsToAdd+2)), " Expression")
names(randData) = newNames
newNoiseCols    = dplyr::bind_cols(randData)
addedNoiseData  = bind_cols(exampleData, newNoiseCols)

#if you want to drop columns:
noGeneFive      = addedNoiseData %>% select(-contains("Gene 5"))

#remember that the data is in
dataforClustering

#######################################
###
###   UP TO YOU FROM HERE ON OUT!
###
#######################################

########
#Answer#
########


#make some empty lists, and a vector of dimensions to add to loop over
noiseDimsToAdd  = c(10, 20, 50, 100, 200, 1000)
dataList        = vector("list", length(noiseDimsToAdd))
pcaDataList     = vector("list", length(noiseDimsToAdd))
kMeansList      = vector("list", length(noiseDimsToAdd))
plotList        = vector("list", length(noiseDimsToAdd))
for (i in seq_along(noiseDimsToAdd)) {
  
  #add random data
  colsToAdd        = noiseDimsToAdd[i]
  randData         = purrr::map(rep(nrow(dataforClustering), colsToAdd), rnorm)
  #randData         = purrr::map(rep(nrow(dataforClustering), colsToAdd),
  #rnorm, sample(seq(0, 8), 1), sample(seq(0, 5), 1))
  newNames         = paste0("Gene ", seq(3,(colsToAdd+2)), " Expression")
  names(randData)  = newNames
  newNoiseCols     = dplyr::bind_cols(randData)
  addedNoiseData   = bind_cols(dataforClustering, newNoiseCols)
  
  dataList[[i]]    = addedNoiseData
  
  #do pca
  pcaThisData      = prcomp(addedNoiseData %>% select(-contains("True Cluster")),
                           center = TRUE, scale. = TRUE )
  
  pcaDataList[[i]] = pcaThisData
  
  #run kmeans
  kMeansThisData   = kmeans(addedNoiseData %>% select(-contains("True Cluster")), centers = 4)
  
  kMeansList[[i]]  = kMeansThisData
  
  #make plots
  plotTrueCluster   = ggbiplot::ggbiplot(pcaThisData, groups = as.factor(dataforClustering$`True Cluster`), varname.adjust = 1.1, varname.abbrev = TRUE, var.axes = ifelse(noiseDimsToAdd[i]<100, TRUE, FALSE)) +
    theme_bw() +
  ggtitle(paste0("Actual clusters PCA with ", noiseDimsToAdd[i], " noise dimensions"))
  
  plotKMeansCluster = ggbiplot::ggbiplot(pcaThisData, groups = as.factor(kMeansThisData$cluster), varname.adjust = 1.1, varname.abbrev = TRUE, var.axes = ifelse(noiseDimsToAdd[i]<100, TRUE, FALSE)) +
    theme_bw() +
  ggtitle(paste0("K-means clusters PCA with ", noiseDimsToAdd[i], " noise dimensions"))
  
  plotList[[i]] = list(trueSignalClusters = plotTrueCluster,
                       kMeansClusters     = plotKMeansCluster)
}


show(plotList)

```


Q10: What do you see as noise increases? Try to focus on a group of points and see how they shift colour when you jump between the 'true' clusters (that is, based on the signal we explicitly put in) and the ones k-means finds.

#Answer: Though it is somewhat hard to see, in the beginning the clusters are still the same, even though with more dimensions the 2D plot is looking less and less like actual defined clusters. Once we get to 100 or 200 noise dimensions, they are not the same anymore.

Q11: Contrast the PCAs of the high-dimensional space with those we made before when adding only 1 or 2 noise dimensions. Why are the points so much more spread out?

#Answer: in actuality, every point is so noisy that everything is just far apart from each other in the 200D space. With 200 noise values, the signal is tiny compared to the part that random noise adds to the distance calculations.

Q12: It might seem contrived that we are straight-up adding noise. Of course that would make clustering perform worse, we are diluting the signal by a factor 100 in the end! However, it is not so far off from reality. What biological experiments can you think up that might have similar problems?

#Answer: any experiment where we measure expression of all/many genes, be it in RNA, at the protein level will have this problem: only a small part of the genes will be involved. Also when we measure cell-wide chromatin states, or many phenotypic characteristics in different growth conditions: there is little signal, and a lot of noise. Clustering works to find some patterns, but it's always a dialogue where you need to see what that structure might correspond to, etc.


#####
##
## I DON'T YET KNOW WHETHER I REALLY THINK THE 200-D PLOT SHOWS WHAT I WANT IT TO SHOW, NAMELY THAT MORE AND MORE NOISE MAKES K-MEANS MORE AND MORE PRECARIOUS! FOR ME, SEEMS LIKE IT STILL WORKS WELL EVEN WITH 200 OR 1000 NOISE DIMENSIONS. WHAT GIVES!?
##
#####


#something about hierarchical clustering





































___________________________________________________________________________________________________________
___________________________________________________________________________________________________________



#part 2 eigenface introduction to PCA
Now, let's use something that our brains are especially well-wired to interpret and think about: faces.

The question is: can we take pictures of human faces, run them through dimensionality reduction, and still keep most of the, well, face-ness intact? Start by running the code below to import images of faces (specifically, from here: http://vis-www.cs.umass.edu/lfw/ , and within that data only those people whose names start with a).

```{r}


#get the file names
faceDir = "./Faces"
fileNames = list.files(path =faceDir, pattern=".*.jpg")
str(fileNames)

#read in the images
imageList = lapply(paste0(faceDir, "/", fileNames), readImage)
str(imageList)



```

Great, we have images. Let's view some.

```{r}

#display 15 images
for (i in seq(15)) {
  
  randomImageIndex = sample(seq_along(imageList), 1)
  imageToDisplay = imageList[[randomImageIndex]]
  nameImage      = fileNames[randomImageIndex]
  print(nameImage)
  display(imageToDisplay, "raster")
  
}

```


For our dimensionality reduction to be more manageable, let's make the images grayscale.

```{r}
greyImageList = list(length(imageList))
for (i in seq_along(imageList)) {
  

  greyImageList[[i]] <- imageData(channel(imageList[[i]], "gray"))
  
}


```

See how that looks.

```{r}
for (i in seq(10)) {
  
  randomImageIndex = sample(seq_along(greyImageList), 1)
  imageToDisplay = greyImageList[[randomImageIndex]]
  nameImage      = fileNames[randomImageIndex]
  print(nameImage)
  display(imageToDisplay, "raster")
  
}
```

Now, up to you to question the data somewhat. Answer the following questions using code you write below:

Q1: How many data points are there?
Q2: How many unique people are in this dataset?
Q3: Whose face is most represented in this dataset, and how many times is that face represented?
Q4: What does the distribution of counts of people look like? (use hist)

**Hint: you can use functions like str_replace_all to change '.jpg' into an empty space ''. If you do the same for numbers and underscores, you are left with only the names. Then you can use the unique() function and table() along with which.max() to get the answers**

**Hint: to make a histogram, use R's built-in hist() function**

```{r}

#demonstrations of the functions you can use:


#replace numbers and underscores

someString = "Billy_Jean_Is_Not_my_Love_995599.mp3"
noUnderscores = str_replace_all(someString, "_", "")
noExtension = str_replace_all(noUnderscores, "\\.mp3$", "")
noNumbers = str_replace_all(noExtension, "[0-9]", "")
#You can also use other methods like str_extract or intricate regexp queries, if you know them. If not, these 3
#ingredients should get the result you want!


#investigating how many entries there are of specific types and unique entries

someVector = c("banana", "apple", "grape", "apple", "plum", "banana", "apple", "Weird Al", "Stegosaurus", "plum")
print(someVector)
#get unique entries
print(unique(someVector))
#count and tabulate entries
print(table(someVector))
#get the index of the element in the table with the maximum value 
print(which.max(table(someVector)))
#print that element
print(table(someVector)[which.max(table(someVector))])


#Now it's up to you, get the names of the people whose faces are in the data, see how many unique people
#there are, tally how many times each person is in there and make a histogram, and see who is in there the most

##ANSWER##

print(paste0("Amount of data points: ", length(fileNames)))
namesWithUnderscore = str_extract(fileNames, "[A-Za-z-]*_*[A-Za-z-]*_*[A-Za-z-]*_*[A-Za-z-]*_*")
namesNoUnderscore = str_replace_all(namesWithUnderscore, "_", "")
length(unique(namesNoUnderscore))
tableNames = table(namesNoUnderscore)
mostEntriesIndex = which.max(tableNames)
print(paste0("Most entries: ", tableNames[mostEntriesIndex], " ", names(tableNames[mostEntriesIndex])))
hist(tableNames)

```




Now that you've gotten to know the data some, let's do what we set out to do. First, let's think about the dimensionality of the input space. You can use the code block below to answer these questions.

Q5: What do the numbers printed in the matrix signify? 

#Answer: grayscale values, from 0-1 (actual pixel intensity is normally from 0-255)
Q6: What is the dimension of the input space?
#Answer 250*250 = 62500-dimensional
Q7: Do you think that this n*n-dimensional space is evenly (or randomly) filled with examples from this data, or not? In other words: in all the values that data in this n.n-dimensional space could take, how special are faces? Why?
#Answer: In this dimensionality, we can represent images of wasps, chickens, buildings, galaxies, UFOs, and what have you. It is a huge possibility space. If we think about this huge space, it makes sense that faces are a very special case within all the possible values. Another way of conceptualising this is thinking about a random point in such a space: an image of random values between 0-1 for each pixel will look nothing like a face at all. Hence, nor like a fish or a boat or a house, while those things in turn look nothing like a face. 

######
##PERHAPS MAKE HERE A RANDOM GRAYSCALE IMAGE so they can see that it looks nothing like that! 
#######
Q8: Do you think it is a logical step to perform PCA, a linear dimension reduction method, on this (type of) data?

#Answer: No, not really. You have heard about manifold methods, and given the question above, it would be more logical to try one of those manifold methods to find the subspace in these 62500 dimensions in which to best represent faces. So why do we do this? One, it is a classic example where you can see what PCA is doing. Two: it will show you that even though the linear approach might not seem the most logical, it can still do the job quite well if necessary. Don't be afraid to just use PCA to see what you get in exploratory data analysis, especially before moving on to (much) more complicated methods!

##


```{r}
someImage = sample(greyImageList, 1)[[1]]
print(someImage[1:50, 1:50])
dim(someImage)

```




Regardless of your feelings in question 8, we will soldier on and do the PCA. Remember that PCA does not, in and of itself, do dimension reduction: it just creates orthogonal axes through your data, starting from the axis that captures most variance, to the next most variance, etc. In other words: there are as many of these axes as there are dimensions in your data. It's up to you to select the top n principal components to keep. As a first step, we need to get our data in a form useful for PCA. PCA is often performed via something called singular value decomposition (details for the mathematically inclined here: https://stats.stackexchange.com/questions/189822/how-does-centering-make-a-difference-in-pca-for-svd-and-eigen-decomposition ), and this requires data with a zero-mean. This has a cool side effect: we need to calculate the 'average face' and subtract it from the data. Up to you to do this and uncover the ghostly mean face!

Q9: in the code block below, compute the mean value of every pixel over all images. Then, visualise this face by displaying the matrix.
Q10: in the code block below, make sure each row has 0 mean by subtracting the rowMeans from the data. (So you subtract the mean grey intensity of pixel 1 across all images from pixel 1 across all images). To check that you did it correctly, calculate the rowMeans() again. They should be zero (or close to it: e^(-17) is close enough). 

**Hint: use matrix() (as shown below), rowMeans(), and display(YOURAVERAGEFACEHERE, method = "raster")**

```{r}

#step 1: each image is a 250*250 square matrix. But for PCA we need something like a data table, with samples in the columns, and values in the rows, i.e. like:

exampleData = data.frame("face1" = c( 0.55,0.33,0,0.56), "face2" = c(0.786,0.257,1, 0))
rownames(exampleData) = c("pixel1", "pixel2", "pixel3", "pixel62500")
exampleData

#You can easily switch between a vector and a matrix
as.vector(someImage)
matrix(as.vector(someImage), nrow = 250, ncol = 250)

#put every image in a data frame
greyImageDataFrame = data.frame(matrix(nrow = length(someImage), ncol = length(fileNames)))
for (imageMatrixIndex in seq_along(greyImageList)) {
  
  greyImageDataFrame[,imageMatrixIndex] = as.vector(greyImageList[[imageMatrixIndex]])
  
}
rownames(greyImageDataFrame) = paste0(rep("pixel", length(someImage)), seq(1,length(someImage)))
colnames(greyImageDataFrame) = paste0(rep("face", length(imageList)), seq(1,length(imageList)))
#over to you. We now have a 62500*1054 data.frame with pixel data. Calculate the average face, display it, and subtract the average values from the data.


##ANSWER##

#Computing the 'average' face in these datasets.
avgFace = rowMeans(greyImageDataFrame)

#So, how does an average grayscale face look?

avgFaceMatrix = matrix(avgFace, nrow = 250, ncol = 250)
display(avgFaceMatrix, "raster")

#Subtracting the average face
meanSubtractedGreyImageDataFrame = greyImageDataFrame - avgFace

```


Okay, with that done we can do our PCA. We'll pare our set down to 256 faces to speed up calculation. The beauty of this approach is that we can visualise what, exactly, PCA is doing. Normally, we cannot. Go ahead and run the following code. In it, we perform PCA, and then look at the 10 eigenvectors with the largest eigenvalues, or in less technical terms: the 10D representation of the data that captures most of the variance. Since each eigenvector is 62500-dimensional, you can simply put it back into the form of an image and look at it, to see what each image represents. In other words: you can look at the largest variance from the mean face, and then the second largest, etc. in this dataset. This allows you to visually inspect what each principal component might represent.


```{r, fig.width=7, fig.height=7}

#step 1: we'll randomly remove 75% of the images: otherwise things will take a LONG time to calculate

sampledFaceIndices = sample(seq(1:1024), 1024/4)
smallerDataFrame = meanSubtractedGreyImageDataFrame[, sampledFaceIndices]



facePCA = prcomp(smallerDataFrame, scale = TRUE)
print(summary(facePCA))

#What do the principal components look like? What does a high or low value on these principal components correspond to? Let's find out!

#get everything in matrix form
fullComponentMatrixList = list(ncol(facePCA$x))
for (component in seq(1:ncol(facePCA$x))) {
  
  fullComponentMatrixList[[component]] = matrix(facePCA$x[, component], nrow = 250, ncol = 250)
}



for (component in seq(1:10)) {
  
  #draw it --> drawing logic assumes a different ordering of the matrix so we have to flip it, see: https://stackoverflow.com/a/66453734 
  image(fullComponentMatrixList[[component]][, nrow(fullComponentMatrixList[[component]]):1],
        col = gray.colors(256, 0, 1))
  
}

```

In these images, the greyer an area, the larger its importance on this Principal component.##IS THIS TRUE?

Q11: First, scroll up to look at the normal input faces we fed into our PCA again. Given the variability in these faces (and backgrounds!), what do you think the first five principal components might roughly correspond to? Think of orientation (left-right and where the face is in the image), background brightness, and some prominent facial features. (Note: this question is hard to do wrong, but also hard to do exactly right).

#Answer: it seems that at least one dimension corresponds to the lighting of the background around the faces, and others might focus in on hair.

Q12: The first principal component is the dimension along which faces in this dataset vary the most from the mean. Why do you think that it looks as it does?

##Answer: It is probable that PC1, at least, has something to do with the background brightness

PCA is dimensionality reduction. So let's see: in how many dimensions can we put our data to still capture most of the variance? Run the following code and look at the plots, then answer the questions below.


```{r}

#How much variance in these faces does each principal component have?

#calculate total variance explained by each principal component
varExplained = facePCA$sdev^2 / sum(facePCA$sdev^2)

#create scree plot
qplot(c(1:length(varExplained)), varExplained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot (percentage of variance explained by each PC)") +
  ylim(0, 1) +
  theme_bw()

#Create cumulative variance explained plot
cumulVar <- cumsum(varExplained)
plot(cumulVar[0:50], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot (zoom-in until 75%)")
abline(h = 0.5, col="blue", lty=5)
abline(v = 10, col="blue", lty=5)
abline(h = 0.75, col="red", lty=5)
abline(v = 40, col="red", lty=5)
legend("bottomright", legend=c("Cut-off @ PC10", "Cut-off @ PC40"),
       col=c("blue", "red"), lty=5, cex=0.6)


#what if we really wanted 90% of the variance?
plot(cumulVar[0:100], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot (up to 90%)")
abline(h = 0.9, col="green", lty=5)
abline(v = 100, col="green", lty=5)

legend("bottomleft", legend=c("Cut-off @ 100"),
       col=c("green"), lty=5, cex=0.6)


#print(facePCA$center)
#print(facePCA$stdev)


```
Q13 How many dimensions do you need to keep 50% of the variance in the face data? And 75%?

Q14 By what factor can you decrease the dataset size and still keep 90% of the variance? Remember, you started with 256 faces, each of which needed 62500 datapoints to describe. But by rotating things around, you can shrink from 256 to fewer values to describe this data, and that still represents 90% of the variation in the original 256 unique images.

***A: you compressed the data by a factor of 2.56 ***


Now what we can do is try to reconstruct our original faces from the values they have in the compressed space (and by adding the mean face back in). Let's see how that looks for a changing number of principal components used. Run the two code blocks below and look at the results.



```{r, fig.width=7, fig.height=7}








#take a random face from the 256 that we did the PCA on
randomImageIndex = sample(sampledFaceIndices, 1)
faceNumberInPCAObject = which(sampledFaceIndices == randomImageIndex)
imageToDisplay = greyImageList[[randomImageIndex]]
nameImage      = fileNames[randomImageIndex]



#make more specific faces continuously, by adding more and more PCs

steps = c(1,2,3,4,5,6,7,8,9,10, seq(20, 240, 20), 256)

image(avgFaceMatrix[, nrow(avgFaceMatrix):1],col = gray.colors(256, 0, 1),
        main = paste0("Average Face"))
for (PCsToAdd in steps[seq_along(steps)]) {
  
  currentReconstruction = avgFaceMatrix
  for (matrixIndex in seq_along(fullComponentMatrixList[1:PCsToAdd])) {
    currentReconstruction = currentReconstruction + facePCA$rotation[faceNumberInPCAObject, matrixIndex] *
      fullComponentMatrixList[1:PCsToAdd][[matrixIndex]]
  }
  
  
image(currentReconstruction[, nrow(currentReconstruction):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Reconstruction using ", PCsToAdd, " Principal Components" ))
  
}

#print normal face
print(nameImage)
image(imageToDisplay[, nrow(imageToDisplay):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Original" ))



```


Note that the slight discrepancy between the 256-PC reconstruction and the original image is because of a normalisation of the variance in every pixel: if pixel 1 only ever has values between 0.6 and 0.7, while another can vary between 0.2 and 1, then the second pixel would automatically have 'more' variance, so influence the PCA more, but that is not, per se, fair. 


We want to use PCA not to keep all dimensions and reconstruct things. That may be fun, but it is also silly. How about actually reducing the dimensions? Can we see anything in the 2D representation of this data? Where, for example, is our most numerous gentleman, Ariel Sharon, in this representation? 

```{r}

imageIndicesSharon = which(namesNoUnderscore == "ArielSharon")
sampledDataPointsSharon =  which(sampledFaceIndices %in% imageIndicesSharon)

dataWithSharonNotSharon = data.frame(facePCA$rotation[,1:2])
dataWithSharonNotSharon$Person = "Not Sharon"
dataWithSharonNotSharon[sampledDataPointsSharon,"Person"] = "Sharon"

plot2D = ggplot(data =dataWithSharonNotSharon,
           aes(x = PC1, y = PC2, color = Person)) +
           geom_point() +
           theme_bw()

show(plot2D)

```

Seems like Ariel Sharon is hanging out mostly in one area. In fact, it seems that Ariel Sharon is mostly to one side of PC1. Can we see why? Run the code below, and answer the question.



```{r, fig.width=4, fig.height=4}
#select points based on PC1 coordinates
outlierRows = dataWithSharonNotSharon[sampledDataPointsSharon, ][dataWithSharonNotSharon[sampledDataPointsSharon, ]$PC1 < 0,]
normalRows  = dataWithSharonNotSharon[sampledDataPointsSharon, ][dataWithSharonNotSharon[sampledDataPointsSharon, ]$PC1 >= 0,]

#Becaue the sign of the coordinates on the PC is random, I need to make sure that what I call outliers or not based on PC1 are indeed the outliers. I do this by checking that the outliers are the fewest datapoints, while normal comprises  most of the points.
if (nrow(normalRows) < nrow(outlierRows)) {
  #flip them around
  realNormal = outlierRows
  outlierRows = normalRows
  normalRows = realNormal
  
}

sharonOutlierImageIndices = as.numeric(str_replace_all(rownames(outlierRows), "face", ""))
sharonNormalImageIndices  = as.numeric(str_replace_all(rownames(normalRows), "face", ""))


#plot the images of normal and outlier Sharons based on PC1
for (index in sample(sharonNormalImageIndices, length(sharonNormalImageIndices)/2)) {
  image(greyImageList[[index]][, nrow(greyImageList[[index]]):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Sharon Normal" ))
}

for (index in sharonOutlierImageIndices) {
  image(greyImageList[[index]][, nrow(greyImageList[[index]]):1],
        col = gray.colors(256, 0, 1),
        main = paste0("Sharon Outlier" ))
}






```
Q15 Do you see any obvious differences between the outliers and normal points? Do you think this is logical? How does this reflect on your earlier answer to what PC1 seemed to do?

##Answer: It is hard to tell. It could be that the images differ mostly in the brightness of their background: it looks like the normal images have brighter backgrounds than the outliers. But it is hard to be sure. However, it is very well possible that lightning conditions of the background is a major point of variance. Still, it is hard to tell, and We don't expect that we could accurately judge these images along differences in just 1 principal component that accounts for only ~12% of the variation.##




Now, there's actually quite a lot of variation in the faces in this dataset. Let's quickly check how PCA fares in another dataset with faces pictured from the front and with uniform background.

```{r, fig.width=9.6, fig.height=5}
#read data
centeredFacesDataset = R.matlab::readMat("./CenteredFaces/faces.mat")

#take an example matrix for reference purposes
exampleFaceMatrix = centeredFacesDataset$face[[3]][[1]]

#make the matrix modification into a function so I don't have to do that manually each time
displayCenteredFace <- function(face, title = "") {
  transposedFace = t(face)
  reorderedFaceMatrixForDrawing = transposedFace[, ncol(transposedFace):1]
  image(reorderedFaceMatrixForDrawing, col = gray.colors(256, 0, 1), main = title)
  #don't return anything
  k = invisible(TRUE)
}

#draw 10 example faces
for (i in seq(1:10)) {
  indexToDisplay = sample(seq(1:length(centeredFacesDataset$face)), 1)
  displayCenteredFace(centeredFacesDataset$face[[indexToDisplay]][[1]], title = paste0("Face ", i))
  
}


#put every image in a data frame as a 1728 by 1 vector.
greyImageDataFrameCentered = data.frame(matrix(nrow = length(exampleFaceMatrix), ncol = length(centeredFacesDataset$face)))
for (imageMatrixIndex in seq_along(centeredFacesDataset$face)) {
  
  greyImageDataFrameCentered[,imageMatrixIndex] = as.vector(centeredFacesDataset$face[[imageMatrixIndex]][[1]])
  
}
rownames(greyImageDataFrameCentered) = paste0(rep("pixel", length(exampleFaceMatrix)), seq(1,length(exampleFaceMatrix)))
colnames(greyImageDataFrameCentered) = paste0(rep("face", ncol(greyImageDataFrameCentered)),
                                              seq(1,ncol(greyImageDataFrameCentered)))

#See how that looks
head(greyImageDataFrameCentered)

#subtract the mean face
avgCenteredFace = rowMeans(greyImageDataFrameCentered)
normalisedCenteredFaces = greyImageDataFrameCentered - avgCenteredFace

#do the pca
pcaCenteredFaces = prcomp(normalisedCenteredFaces, scale = TRUE)

#Assemble the top 10 principal components (eigenvectors) into images you can view (back to matrix form)
eigenFacesList = list(10)
for (PCnum in seq_along(colnames(pcaCenteredFaces$x[,1:10]))) {
  eigenFacesList[[PCnum]] = matrix(pcaCenteredFaces$x[, PCnum], nrow = dim(exampleFaceMatrix)[1],
                                 ncol = dim(exampleFaceMatrix)[2])
  
}
#show the average face
displayCenteredFace(matrix(avgCenteredFace, nrow = dim(exampleFaceMatrix)[1],
                                 ncol = dim(exampleFaceMatrix)[2]), title = "Average face")

#show the first 10 PCs
purrr::map2(eigenFacesList, paste0("PC ", seq(1:10)), displayCenteredFace)

```

Q16: what do you think the first few principal components correspond to here? Again, it is hard to be sure so feel free to speculate.

##Answer: PC1 and PC2 both seem to focus on hair, PC3 could be about where, exactly, the shoulders are in the image, PC4 is just a scary lizardman put there by our overlords to scare us (real answer: no idea), PC5 might focus a bit on the type of collar/neck clothing.

##let the students take a picture of themselves. --> make a face dataset out of that?
## 




So far so good. We hope this has given you a bit more intuition about what PCA might be doing. Important takeaways are that:
1. Even if you would think that manifold methods are the method of choice (faces are a very special case of n*n images, so you could probably represent them better in some reduced space specifically designed to pull them apart, i.e. the manifold containing faces) a linear method can still work well: with 100-150 PCs, you got a pretty good reconstruction of the face already, so you can reduce to this dimensionality and not lose too much discriminative power!
2. PCA, on its own, is not a dimensionality reduction tool: running PCA just gives us the linearly rotated original space. It is up to us to select how much of the variance of the data we want to keep/think is enough for visualisation.
3. It is not a given that a principal component is interpretable to us, or maps to a specific thing in the data: we got lucky that PC1 might correspond to something we can understand. Many of the PCs just look very strange.



























This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
